# Optimizaci√≥n de Tokens - Proyecto Chi Evolution
## Fecha: 2025-02-03

---

## 1. COMPRESI√ìN DE PROMPTS

### El Problema del Contexto Largo

**Datos clave:**
- GPT-4 muestra degradaci√≥n de 15-47% en performance con contextos largos
- Fen√≥meno "lost in the middle": informaci√≥n en el centro del contexto se pierde
- A 32,000 tokens, 11 de 12 modelos testeados bajan de 50% de performance
- Costo: $2.50-$5.00 por mill√≥n de tokens

**Ejemplo de impacto econ√≥mico:**
- 3 mil millones de tokens/mes con Claude 4 Opus = ~$270,000
- Con 5x compresi√≥n = $54,000 (ahorro de $216,000/mes)

---

### T√©cnicas de Compresi√≥n

#### A. LLMLingua (Microsoft Research)

**Estado del arte en compresi√≥n de prompts.**

| Caracter√≠stica | Detalle |
|----------------|---------|
| Compresi√≥n | Hasta 20x |
| P√©rdida de performance | Solo 1.5% |
| M√©todo | Token perplexity con budget controller |

**C√≥mo funciona:**
```python
# Usa un modelo peque√±o para calcular perplexity de tokens
# Tokens con menor perplexity = menos informaci√≥n = eliminables

from llmlingua import PromptCompressor

compressor = PromptCompressor()
compressed = compressor.compress(
    context=long_context,
    instruction="Answer the question based on context",
    question=user_question,
    target_token=300  # Target de tokens comprimidos
)
```

**Budget Controller:**
- Instrucciones: 10-20% compresi√≥n (preservar claridad)
- Ejemplos: 60-80% compresi√≥n (alta redundancia)
- Pregunta: 0-10% compresi√≥n (preservar intenci√≥n)

**LongLLMLingua:** Extensi√≥n para RAG
- Question-aware coarse-to-fine compression
- Document reordering (combatir positional bias)
- +21.4% performance en NaturalQuestions
- 94% reducci√≥n de costo en benchmark LooGLE

---

#### B. Compresi√≥n Extractiva vs Abstractiva

| Tipo | M√©todo | Cu√°ndo usar |
|------|--------|-------------|
| **Extractiva** | Seleccionar oraciones verbatim | RAG, c√≥digo, datos estructurados |
| **Abstractiva** | Generar nuevo resumen | Summarization puro |

**Resultados 2024:**
- Extractiva reranker: +7.89 F1 points a 4.5x compresi√≥n
- Abstractiva: -4.69 F1 points a ratio similar

**Conclusi√≥n:** Para RAG, extractiva suele ser mejor (filtra ruido).

---

#### C. Keyphrase Extraction

**Algoritmos principales:**

| Algoritmo | Tipo | F1 Score | Velocidad |
|-----------|------|----------|-----------|
| **RAKE** | Estad√≠stico | 32% | Muy r√°pido |
| **YAKE** | Estad√≠stico | 36% | R√°pido |
| **TextRank** | Graph-based | 36% | Media |
| **KeyBERT** | Transformer | 40-45% | Lenta (necesita GPU) |

**Uso:** Mantener solo contenido con keyphrases importantes.

---

#### D. Semantic Chunking

**Estrategias:**

1. **Recursive Character Splitting** (LangChain)
```python
from langchain_text_splitters import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=512,
    chunk_overlap=77,
    separators=["\n\n", "\n", ". ", " ", ""]
)
```

2. **Semantic Similarity Splitting**
   - Crear grupos de oraciones
   - Calcular embeddings de cada grupo
   - Insertar breaks donde similarity < threshold

3. **Proposition-based** (Dense X Retrieval)
   - Decomponer en proposiciones at√≥micas
   - Reemplazar pronombres con entidades completas
   - +5.9 a +7.8 EM@100 en question-answering

**Hallazgo Vectara 2024:**
Semantic chunking raramente justifica su costo computacional.
Fixed-size chunking igual√≥ o super√≥ m√©todos sem√°nticos en 3/5 datasets.

---

### Gu√≠a de Selecci√≥n de T√©cnica

| Caso de Uso | T√©cnica Recomendada | Compresi√≥n |
|-------------|---------------------|------------|
| RAG multi-documento | Extractiva con reranker | 2-10x |
| Chain-of-thought | LLMLingua | 5-20x |
| C√≥digo / SQL | Extractiva (nunca token pruning) | 2-5x |
| Summarization puro | Abstractiva | 5-10x |
| Chat history | Sliding window + summarization | Variable |

---

## 2. T√âCNICAS DE RESUMEN DE CONTEXTO

### Estrategias de Memoria Conversacional

#### A. Sliding Window (Ventana Deslizante)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Mensajes antiguos ‚Üí [Summary] ‚Üí Mensajes recientes (N)     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Configuraci√≥n t√≠pica:**
- Mantener √∫ltimos 8-10 mensajes completos
- Resumir mensajes antiguos en summary
- Cuando se excede l√≠mite, regenerar summary

**Implementaci√≥n:**
```python
# Ejemplo con LangChain
from langchain.memory import ConversationSummaryBufferMemory

memory = ConversationSummaryBufferMemory(
    llm=llm,
    max_token_limit=1000,
    return_messages=True
)
```

---

#### B. Jerarqu√≠a de Memoria

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CORE MEMORIES (Identidad, preferencias cr√≠ticas)           ‚îÇ
‚îÇ  ‚îî‚îÄ Nunca se resumen, siempre disponibles                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  LONG-TERM MEMORY (Todas las conversaciones)                ‚îÇ
‚îÇ  ‚îî‚îÄ Acceso por retrieval sem√°ntico                          ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  SHORT-TERM MEMORY (√öltimas 24-48h)                         ‚îÇ
‚îÇ  ‚îî‚îÄ Summary + √∫ltimos mensajes                              ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  WORKING MEMORY (Contexto inmediato)                        ‚îÇ
‚îÇ  ‚îî‚îÄ Ventana deslizante de N mensajes                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

#### C. Consolidaci√≥n Peri√≥dica

| Frecuencia | Acci√≥n |
|------------|--------|
| **Nightly** | Comprimir conversaciones del d√≠a en daily summary |
| **Weekly** | Identificar patrones, actualizar core memories |
| **Monthly** | Archivar memorias de bajo acceso, re-indexar |

---

## 3. ESTRATEGIAS DE CACH√â SEM√ÅNTICA

### GPTCache

**Librer√≠a l√≠der para cach√© sem√°ntico de LLMs.**

**Beneficios:**
- üí∞ Reduce costos hasta 10x
- ‚ö° Mejora velocidad hasta 100x
- üîÑ Integraci√≥n con LangChain, LlamaIndex

**Arquitectura:**
```
User Query ‚Üí Embedding ‚Üí Vector Search ‚Üí Similarity Check ‚Üí Cache Hit?
                ‚Üì                                       ‚Üì
           Vector Store                          LLM API Call
                ‚Üì                                       ‚Üì
         Similar Queries                        Store Response
```

**Modos de operaci√≥n:**

| Modo | Descripci√≥n |
|------|-------------|
| **Exact Match** | Respuesta exacta almacenada |
| **Similar Search** | B√∫squeda sem√°ntica de queries similares |
| **Temperature** | Control de exploraci√≥n vs explotaci√≥n |

**Integraci√≥n simple:**
```python
from gptcache import cache
from gptcache.adapter import openai

cache.init()
cache.set_openai_key()

# Ahora todas las llamadas a openai usan cach√©
```

**M√©tricas:**
- Hit Ratio: % de queries servidas desde cach√©
- Latency: Tiempo de respuesta desde cach√©
- Recall: % de queries que deber√≠an haber sido servidas desde cach√©

---

### Estrategias de Cach√© para OpenClaw

#### Nivel 1: Cach√© de Respuestas Exactas
```python
# Para queries id√©nticas
key = hash(query)
if key in cache:
    return cache[key]
```

#### Nivel 2: Cach√© Sem√°ntica
```python
# Para queries similares
embedding = embed(query)
similar = vector_search(embedding, threshold=0.95)
if similar:
    return similar.response
```

#### Nivel 3: Cach√© de Contexto
```python
# Para recuperaci√≥n de memoria
context_hash = hash(retrieved_contexts)
if context_hash in context_cache:
    return context_cache[context_hash]
```

---

## 4. ESTRATEGIAS ADICIONALES

### A. Pre-fetching Proactivo

```python
# Predecir informaci√≥n que el usuario necesitar√°
# bas√°ndose en el contexto actual

if topic == "viaje a Par√≠s":
    prefetch(["clima Par√≠s", "mejores restaurantes Par√≠s", "metro Par√≠s"])
```

### B. Streaming Parcial

```python
# Enviar partes de la respuesta mientras se genera
# reduce percepci√≥n de latencia

for chunk in llm.stream(query):
    yield chunk
```

### C. Modelo Cascada

```python
# Intentar con modelo m√°s barato primero

def cascade_query(query):
    # 1. Intentar con modelo local/cheap
    response = cheap_model(query)
    if confidence(response) > 0.9:
        return response
    
    # 2. Escalar a modelo premium
    return expensive_model(query)
```

---

## 5. IMPLEMENTACI√ìN PARA CHI

### Arquitectura Propuesta

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    User Query                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CACH√â LAYER                                                 ‚îÇ
‚îÇ  ‚îú‚îÄ Exact Match Cache (Redis)                               ‚îÇ
‚îÇ  ‚îî‚îÄ Semantic Cache (GPTCache + LanceDB)                     ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì (miss)
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  CONTEXT COMPRESSION                                         ‚îÇ
‚îÇ  ‚îú‚îÄ Memory Retrieval (LanceDB + BGE-M3)                     ‚îÇ
‚îÇ  ‚îú‚îÄ Sliding Window (√∫ltimos 10 mensajes)                    ‚îÇ
‚îÇ  ‚îú‚îÄ Summary Buffer (resumen hist√≥rico)                      ‚îÇ
‚îÇ  ‚îî‚îÄ LLMLingua Compression                                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LLM API CALL                                                ‚îÇ
‚îÇ  ‚îî‚îÄ Modelo seleccionado (cascada si aplica)                 ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                      ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  POST-PROCESSING                                             ‚îÇ
‚îÇ  ‚îú‚îÄ Store in cache                                          ‚îÇ
‚îÇ  ‚îú‚îÄ Update memory                                           ‚îÇ
‚îÇ  ‚îî‚îÄ Stream to user                                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## 6. ESTIMACI√ìN DE AHORRO

### Escenario: Chi con uso moderado

| Componente | Sin Optimizaci√≥n | Con Optimizaci√≥n | Ahorro |
|------------|------------------|------------------|--------|
| Tokens LLM | 50M/mes | 15M/mes | 70% |
| Costo API | $250/mes | $75/mes | $175/mes |
| Latencia avg | 2s | 500ms | 75% |
| Cache Hit Rate | 0% | ~60% | - |

---

## 7. CONCLUSIONES

1. **LLMLingua** es la t√©cnica m√°s efectiva para compresi√≥n (20x con 1.5% p√©rdida)
2. **Sliding window + summary** es el est√°ndar para memoria conversacional
3. **GPTCache** puede reducir costos hasta 10x con implementaci√≥n simple
4. **Compresi√≥n extractiva > abstractiva** para RAG y c√≥digo
5. **Semantic chunking** raramente justifica su costo vs fixed-size
6. **Jerarqu√≠a de memoria** (working/short/long/core) optimiza retrieval

---

## 8. REFERENCIAS

- [LLMLingua (Microsoft Research)](https://github.com/microsoft/LLMLingua)
- [LongLLMLingua Paper](https://aclanthology.org/2024.acl-long.91/)
- [GPTCache (Zilliz)](https://github.com/zilliztech/GPTCache)
- [Stanford - Lost in the Middle](https://arxiv.org/abs/2307.03172)
- [Prompt Compression Survey](https://arxiv.org/html/2410.12388v2)

---

*Documento generado para Proyecto Chi Evolution - Fase 1*
